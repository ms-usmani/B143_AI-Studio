{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Binary Classification of Bank Marketing Using Tsetlin Machine**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Introduction**\n",
    "\n",
    "This report describes the process of implementing a binary classification task using the Tsetlin Machine (TM) algorithm. We will utilize the \"Bank Marketing Dataset\" from the UCI Machine Learning Repository for this purpose. This data in this dataset contains information related to direct marketing campaigns, with the aim of prediction of a term deposit whether a client will receive or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset Selection and Preprocessing**\n",
    "\n",
    "1. Description of Dataset\n",
    "    The \"Bank Marketing Dataset\" comprises 41,188 records and 20 features. The target variable is \"y\" indicates if a term deposit is subscribed by client.\n",
    "2. Features\n",
    "    The features include various client traits and the results of marketing campaigns, started from Age, job and ended with last variable of Number of Employes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocessing Steps**\n",
    "\n",
    "    1.\tLoading the Dataset: Using Pandas, the dataset was loaded.\n",
    "    2.\tHandling Categorical Variables: Categorical variables were encoded by one-hot encoding.\n",
    "    3.\tFeature and Target Extraction: The target variable \"y\" was extracted, and the remaining columns were employed as features.\n",
    "    4.\tNormalization: StandardScaler was used to modify features to ensure they are on a similar scale.\n",
    "    5.\tTrain-Test Split: The data was splited into two parts training and testing where training consume 80% while testing 20%.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implementation of Tsetlin Machine**\n",
    "\n",
    "**Necessary Steps**\n",
    "\n",
    "**Binarization**\n",
    "\n",
    "The features were based on their median values to transform them into binary forms, which is a requirement for the Tsetlin Machine.\n",
    "\n",
    "**Genetic Algorithm for Feature Selection**\n",
    "\n",
    "A placeholder genetic algorithm was employed for feature selection. It ranked features based on their absolute correlation with the target variable and selected the highest 75% of the features.\n",
    "\n",
    "**Hyperparameter Tuning**\n",
    "A grid search was conducted to locate the optimal hyperparameters for the Tsetlin Machine. The parameters tuned included:\n",
    "    •\tClauses\n",
    "    •\tThreshold (T)\n",
    "    •\tSpecificity (s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training of Tsetlin Machine**\n",
    "\n",
    "The Tsetlin Machine was initially developed with the best hyperparameters found from the grid search and trained on the binarized training data for 100 epochs.\n",
    "\n",
    "**Evaluation**\n",
    "\n",
    "Following metrics was used to evaluate the performance of model:\n",
    "    •\tAccuracy of model\n",
    "    •\tPrecision of model\n",
    "    •\tRecall of model\n",
    "    •\tF1-Score "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comparison of Model**\n",
    "\n",
    "**Conventional Models Implementation**  \n",
    "\n",
    "For comparison the following conventional models were implemented:\n",
    "    •\tRandom Forest Model (RF)\n",
    "    •\tLogistic Regression Model (LR)\n",
    "    •\tSupport Vector Machine \n",
    "    •\tGradient Boosting Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Performance Metrics**\n",
    "\n",
    "Each model was trained on the chosen features and evaluated using the same metrics as the Tsetlin Machine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results**\n",
    "\n",
    "1. Tsetlin Machine\n",
    "    •\tAccuracy of model: 0.9018\n",
    "    •\tPrecision of model: 0.7059\n",
    "    •\tRecall of model: 0.2310\n",
    "    •\tF1-Score of model: 0.3481\n",
    "2. Random Forest \n",
    "    •\tAccuracy of model: 0.9118\n",
    "    •\tPrecision of model: 0.6444\n",
    "    •\tRecall of model: 0.4963\n",
    "    •\tF1-Score of model: 0.5607\n",
    "3. Logistic Regression\n",
    "    •\tAccuracy of model: 0.9113\n",
    "    •\tPrecision of model: 0.6711\n",
    "    •\tRecall of model: 0.4278 \n",
    "    •\tF1-Score of model: 0.5225\n",
    "4. SVM\n",
    "    •\tAccuracy of model: 0.9081\n",
    "    •\tPrecision of model: 0.6732 \n",
    "    •\tRecall of model: 0.3701\n",
    "    •\tF1-Score of model: 0.4776\n",
    "5. Gradient Boosting\n",
    "    •\tAccuracy of model: 0.9200\n",
    "    •\tPrecision of model: 0.6885\n",
    "    •\tRecall of model: 0.5390\n",
    "    •\tF1-Score of model: 0.6047 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comparison Analysis** \n",
    "\n",
    "**Strengths and Weaknesses of Tsetlin Machine**\n",
    "\n",
    "Strengths:\n",
    "\n",
    "    o\tThe Tsetlin Machine is interpretable and provides a rule-based approach to classification.\n",
    "    o\tIt achieved a competitive advantage in terms of accuracy compared to traditional models.\n",
    "\n",
    "Weaknesses:\n",
    "\n",
    "    o\tThe precision and recall were lower than other models, indicating potential problems with handling imbalanced datasets.\n",
    "    o\tBinarization and hyperparameter tuning  is required for the Tsetlin Machine, which resulted in the difficulty of the implementation.\n",
    "\n",
    "**Comparison with Gradient Boosting Classifier**\n",
    "\n",
    "Strengths:\n",
    "\n",
    "    o\tBetter performance in terms of precision, precision, recall, and F1-score.\n",
    "    o\tRobustness to overfitting.\n",
    "\n",
    "Weaknesses:\n",
    "\n",
    "    o\tLess interpretable than the Tsetlin Machine.\n",
    "    o\tComputationally intensive, particularly for large datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion**\n",
    "\n",
    "The Tsetlin Machine achieved a competitive accuracy. The interpretation of the Tsetlin Machine makes it a treasured tool for applications where considerate the decision-making process is vital. Enhancing Tsetlin Machine's ability to handle imbalanced datasets and further improving hyperparameters could be focused in future work.\n",
    "\n",
    "**Evidence of Interpretability**\n",
    "\n",
    "The rule-based nature of Tsetlin Machine's allows for accurate interpretation of its decision-making process. Each clause in the Tsetlin Machine corresponds to a logical expression that can be directly observed, offering clear insights into how predictions are executed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First few rows of the dataset:\n",
      "   age        job  marital    education  default housing loan    contact  \\\n",
      "0   56  housemaid  married     basic.4y       no      no   no  telephone   \n",
      "1   57   services  married  high.school  unknown      no   no  telephone   \n",
      "2   37   services  married  high.school       no     yes   no  telephone   \n",
      "3   40     admin.  married     basic.6y       no      no   no  telephone   \n",
      "4   56   services  married  high.school       no      no  yes  telephone   \n",
      "\n",
      "  month day_of_week  ...  campaign  pdays  previous     poutcome emp.var.rate  \\\n",
      "0   may         mon  ...         1    999         0  nonexistent          1.1   \n",
      "1   may         mon  ...         1    999         0  nonexistent          1.1   \n",
      "2   may         mon  ...         1    999         0  nonexistent          1.1   \n",
      "3   may         mon  ...         1    999         0  nonexistent          1.1   \n",
      "4   may         mon  ...         1    999         0  nonexistent          1.1   \n",
      "\n",
      "   cons.price.idx  cons.conf.idx  euribor3m  nr.employed   y  \n",
      "0          93.994          -36.4      4.857       5191.0  no  \n",
      "1          93.994          -36.4      4.857       5191.0  no  \n",
      "2          93.994          -36.4      4.857       5191.0  no  \n",
      "3          93.994          -36.4      4.857       5191.0  no  \n",
      "4          93.994          -36.4      4.857       5191.0  no  \n",
      "\n",
      "[5 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from pyTsetlinMachine.tm import MultiClassTsetlinMachine\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"bank-additional-full.csv\", sep=';')\n",
    "\n",
    "# Display the first few rows of the data to confirm the structure\n",
    "print(\"First few rows of the dataset:\")\n",
    "print(data.head())\n",
    "\n",
    "# Encode categorical variables\n",
    "data = pd.get_dummies(data, drop_first=True)\n",
    "\n",
    "# Extract features and target variable\n",
    "X = data.drop(columns=['y_yes'])\n",
    "y = data['y_yes']\n",
    "\n",
    "# Normalize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found:  {'clauses': 200, 'T': 10, 's': 4.5}\n"
     ]
    }
   ],
   "source": [
    "# Genetic Algorithm for feature selection (placeholder function)\n",
    "def genetic_algorithm_feature_selection(X, y):\n",
    "    # Simple feature selection by ranking based on correlation with target\n",
    "    correlations = np.abs([np.corrcoef(X[:, i], y)[0, 1] for i in range(X.shape[1])])\n",
    "    sorted_indices = np.argsort(correlations)[::-1]\n",
    "    selected_features = sorted_indices[:int(0.75 * len(sorted_indices))]  # Select top 75% features\n",
    "    return selected_features\n",
    "\n",
    "# Apply Genetic Algorithm for feature selection\n",
    "selected_features = genetic_algorithm_feature_selection(X_train, y_train)\n",
    "X_train_selected = X_train[:, selected_features]\n",
    "X_test_selected = X_test[:, selected_features]\n",
    "\n",
    "# Binarize the features for Tsetlin Machine\n",
    "X_train_binarized = np.where(X_train_selected > np.median(X_train_selected, axis=0), 1, 0)\n",
    "X_test_binarized = np.where(X_test_selected > np.median(X_test_selected, axis=0), 1, 0)\n",
    "\n",
    "# Hyperparameter tuning\n",
    "param_grid = {\n",
    "    'clauses': [50, 100, 200],\n",
    "    'T': [10, 15, 20],\n",
    "    's': [3.0, 3.9, 4.5]\n",
    "}\n",
    "\n",
    "\n",
    "def grid_search_tsetlin(X_train, y_train, param_grid):\n",
    "    best_params = None\n",
    "    best_score = 0\n",
    "    for clauses in param_grid['clauses']:\n",
    "        for T in param_grid['T']:\n",
    "            for s in param_grid['s']:\n",
    "                tm = MultiClassTsetlinMachine(clauses, T, s)\n",
    "                tm.fit(X_train, y_train, epochs=10)\n",
    "                y_pred = tm.predict(X_train)\n",
    "                score = accuracy_score(y_train, y_pred)\n",
    "                if score > best_score:\n",
    "                    best_score = score\n",
    "                    best_params = {'clauses': clauses, 'T': T, 's': s}\n",
    "    return best_params\n",
    "\n",
    "best_params = grid_search_tsetlin(X_train_binarized, y_train, param_grid)\n",
    "print(\"Best parameters found: \", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TM with Genetic Algorithm - Accuracy: 0.9017965525613013\n",
      "TM with Genetic Algorithm - Precision: 0.7058823529411765\n",
      "TM with Genetic Algorithm - Recall: 0.23101604278074866\n",
      "TM with Genetic Algorithm - F1 Score: 0.34810636583400484\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Initialize and train the Tsetlin Machine with best parameters\n",
    "tm = MultiClassTsetlinMachine(best_params['clauses'], best_params['T'], best_params['s'])\n",
    "tm.fit(X_train_binarized, y_train, epochs=100)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = tm.predict(X_test_binarized)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f\"TM with Genetic Algorithm - Accuracy: {accuracy}\")\n",
    "print(f\"TM with Genetic Algorithm - Precision: {precision}\")\n",
    "print(f\"TM with Genetic Algorithm - Recall: {recall}\")\n",
    "print(f\"TM with Genetic Algorithm - F1 Score: {f1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest - Accuracy: 0.911750424860403, Precision: 0.6444444444444445, Recall: 0.49625668449197863, F1 Score: 0.5607250755287009\n",
      "Logistic Regression - Accuracy: 0.9112648701141054, Precision: 0.6711409395973155, Recall: 0.42780748663101603, F1 Score: 0.5225342913128674\n",
      "SVM - Accuracy: 0.9081087642631707, Precision: 0.6731517509727627, Recall: 0.3700534759358289, F1 Score: 0.4775707384403037\n",
      "Gradient Boosting - Accuracy: 0.920004855547463, Precision: 0.6885245901639344, Recall: 0.5390374331550802, F1 Score: 0.6046790641871626\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Train and evaluate Random Forest with selected features\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train_selected, y_train)\n",
    "y_pred_rf = rf.predict(X_test_selected)\n",
    "\n",
    "# Train and evaluate Logistic Regression with selected features\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train_selected, y_train)\n",
    "y_pred_lr = lr.predict(X_test_selected)\n",
    "\n",
    "# Train and evaluate SVM with selected features\n",
    "svm = SVC()\n",
    "svm.fit(X_train_selected, y_train)\n",
    "y_pred_svm = svm.predict(X_test_selected)\n",
    "\n",
    "# Train and evaluate Gradient Boosting with selected features\n",
    "gb = GradientBoostingClassifier()\n",
    "gb.fit(X_train_selected, y_train)\n",
    "y_pred_gb = gb.predict(X_test_selected)\n",
    "\n",
    "# Evaluation Metrics\n",
    "models = {'Random Forest': y_pred_rf, 'Logistic Regression': y_pred_lr, 'SVM': y_pred_svm, 'Gradient Boosting': y_pred_gb}\n",
    "\n",
    "for model_name, y_pred in models.items():\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    print(f\"{model_name} - Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}, F1 Score: {f1}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
